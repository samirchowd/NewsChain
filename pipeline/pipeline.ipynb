{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "statutory-professor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "import utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "color-combat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [article_id, title, abstract, author, source, time_stamp, link, query]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Loading data\n",
    "\n",
    "# Fake data\n",
    "articles = np.asarray([\n",
    "    [\"Donald Trump buys a dog\", \"Donald Trump recently was found buying a dog\"],\n",
    "    [\"President Trump gets a corgie\", \"The White House welcomes a new furry friend, and its name is Evan!\"],\n",
    "    [\"U.S. Presidential Paws\", \"You won't believe the new dog that Donald and the fam just got!\"],\n",
    "    [\"Biden sells his old car\", \"President Biden just announced that he is selling is '96 Civic\"],\n",
    "    [\"President Biden to sell car\", \"Biden announced via Twitter that he will indeed be selling his car\"],\n",
    "    [\"Biden getting that car cash\", \"Biden is about to cash out once he sells that junker civic!\"]\n",
    "])\n",
    "\n",
    "data = utils.toframe(utils.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "approved-bikini",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>author</th>\n",
       "      <th>source</th>\n",
       "      <th>time_stamp</th>\n",
       "      <th>link</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [article_id, title, abstract, author, source, time_stamp, link, query]\n",
       "Index: []"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "lasting-humanitarian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for computting embeddings:0.017902135848999023\n"
     ]
    }
   ],
   "source": [
    "# Vectorizing \n",
    "roberta = SentenceTransformer('stsb-roberta-base')\n",
    "a = [x + ': ' + y for x,y in zip(data['title'], data['abstract'])]\n",
    "v = utils.encode(a, roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-prototype",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving related articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-bleeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the graph\n",
    "import networkx as nx\n",
    "\n",
    "G = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-dairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.clear()\n",
    "for root, v0 in zip(data['title'], v):\n",
    "    for comp, v1 in zip(data['title'], v):\n",
    "        if utils.doc_sim(v0,v1) > .5:\n",
    "            G.add_edge(root, comp, weight=utils.doc_sim(v0,v1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-worth",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hindu-buddy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms import community\n",
    "from networkx import edge_betweenness_centrality as betweenness\n",
    "\n",
    "def most_central_edge(G):\n",
    "\n",
    "    centrality = betweenness(G, weight=\"weight\")\n",
    "\n",
    "    return max(centrality, key=centrality.get)\n",
    "\n",
    "comp = community.girvan_newman(G, most_valuable_edge=most_central_edge)\n",
    "\n",
    "communities =tuple(sorted(c) for c in next(comp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-sustainability",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting full articles from communities\n",
    "cdf = []\n",
    "for com in communities:\n",
    "    df = pd.DataFrame()\n",
    "    for art in com:\n",
    "        x = data.loc[data['title'] == art]\n",
    "        df = df.append(x)\n",
    "    cdf.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooperative-singapore",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducing each community down to an article \n",
    "min_coms = [] \n",
    "for com in cdf:\n",
    "    min_time = min(com['time_stamp'])\n",
    "    df = pd.DataFrame()\n",
    "    df = df.append(com.loc[com['time_stamp'] == min_time])\n",
    "    min_coms.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-korean",
   "metadata": {},
   "outputs": [],
   "source": [
    "el = nx.Graph()\n",
    "el.clear()\n",
    "\n",
    "# Generating Lv and Huang graph \n",
    "lv = sorted([(x,list(x['time_stamp'])) for x in min_coms], key = lambda x: x[1])\n",
    "lv = [x[0] for x in lv]\n",
    "for i in range(1, len(lv)):\n",
    "    el.add_edge(str(lv[i-1]['title']), str(lv[i]['title']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residential-multimedia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing the graph \n",
    "%matplotlib inline\n",
    "nx.draw_kamada_kawai(el, with_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-graduation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating NewsChain event line \n",
    "\n",
    "# Defining similarity between sub-events \n",
    "# Similarity = avg_pairwise_eos \n",
    "# Rules for drawing edge, if sim > theta, draw directed edge based on min time \n",
    "el.clear()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
